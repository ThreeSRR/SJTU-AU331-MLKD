{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = 'watermelon.csv'\n",
    "df = pd.read_csv(dataroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros((17, 8))\n",
    "labels = np.zeros((17, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = {'青绿': 1, '乌黑': 2, '浅白': 3}\n",
    "feature2 = {'蜷缩': 1, '稍蜷': 2, '硬挺': 3}\n",
    "feature3 = {'浊响': 1, '沉闷': 2, '清脆': 3}\n",
    "feature4 = {'清晰': 1, '稍糊': 2, '模糊': 3}\n",
    "feature5 = {'凹陷': 1, '稍凹': 2, '平坦': 3}\n",
    "feature6 = {'硬滑': 1, '软粘': 2}\n",
    "\n",
    "\n",
    "for row in range(df.shape[0]):\n",
    "    samples[row][0] = feature1[df.iloc[row][1]]\n",
    "    samples[row][1] = feature2[df.iloc[row][2]]\n",
    "    samples[row][2] = feature3[df.iloc[row][3]]\n",
    "    samples[row][3] = feature4[df.iloc[row][4]]\n",
    "    samples[row][4] = feature5[df.iloc[row][5]]\n",
    "    samples[row][5] = feature6[df.iloc[row][6]]\n",
    "    samples[row][6] = df.iloc[row][7]\n",
    "    samples[row][7] = df.iloc[row][8]\n",
    "    labels[row][0] = 1 if df.iloc[row][9] == '是' else 0\n",
    "    \n",
    "labels = np.reshape(labels, -1)\n",
    "flag = np.array([0, 0, 0, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据集乱序**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(17))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "samples = samples[idx]\n",
    "labels = labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDiscriminantAnalysis(object):\n",
    "    def __init__(self, sample_dim):\n",
    "        self.sample_dim = sample_dim\n",
    "        \n",
    "    def train(self, training_samples, training_labels):\n",
    "        sample_dim = self.sample_dim\n",
    "        sample_num = training_samples.shape[0]\n",
    "        if sample_dim != training_samples.shape[1]:\n",
    "            raise Exception(\"Input samples are not compatible with this classifier!\")\n",
    "            \n",
    "        positive = training_samples[training_labels==1]\n",
    "        negative = training_samples[training_labels==0]\n",
    "        \n",
    "        # 计算均值\n",
    "        u0 = negative.mean(0)\n",
    "        u1 = positive.mean(0)\n",
    "        \n",
    "        # 计算协方差\n",
    "        tmp0 = np.zeros_like(negative[0] - u0).dot((negative[0] - u0).T)\n",
    "        for row in negative:\n",
    "            tmp0 += (row - u0).dot((row - u0).T)\n",
    "        sigma0 = 1 / (negative.shape[1] - 1) * tmp0\n",
    "        \n",
    "        tmp1 = np.zeros_like(positive[0] - u1).dot((positive[0] - u1).T)\n",
    "        for row in positive:\n",
    "            tmp1 += (row - u1).dot((row - u1).T)\n",
    "        sigma1 = 1 / (positive.shape[1] - 1) * tmp1\n",
    "        \n",
    "        # 类内离散度矩阵\n",
    "        Sw = sigma0 + sigma1\n",
    "        \n",
    "        if isinstance(Sw, np.float64): \n",
    "            w = (1 / Sw) * (u0 - u1)\n",
    "        else:\n",
    "            w = np.linalg.inv(Sw).dot(u0 - u1)\n",
    "            \n",
    "        negative_center = (w.T).dot(u0)\n",
    "        positive_center = (w.T).dot(u1)\n",
    "        \n",
    "        self.w = w\n",
    "        self.negative_center = negative_center\n",
    "        self.positive_center = positive_center\n",
    "        \n",
    "    \n",
    "    def test(self, testing_samples, testing_labels=None):\n",
    "        \n",
    "        sample_dim = self.sample_dim\n",
    "        sample_num = testing_samples.shape[0]\n",
    "        if sample_dim != testing_samples.shape[1]:\n",
    "            raise Exception(\"Input samples are not compatible with this classifier!\")\n",
    "            \n",
    "        w = self.w\n",
    "        negative_center = self.negative_center\n",
    "        positive_center = self.positive_center\n",
    "        \n",
    "        predicted_labels = np.zeros((sample_num))\n",
    "        \n",
    "        for i in range(sample_num):\n",
    "            xi = testing_samples[i]\n",
    "            \n",
    "            if abs((w.T).dot(xi) - positive_center) > abs((w.T).dot(xi) - negative_center):\n",
    "                predicted_labels[i] = 0\n",
    "            else:\n",
    "                predicted_labels[i] = 1\n",
    "        \n",
    "        if testing_labels is not None:\n",
    "            acc = accuracy_score(testing_labels, predicted_labels)\n",
    "                \n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDAclassifier(training_samples, training_labels, testing_samples, testing_labels):\n",
    "    \n",
    "    sample_dim = len(training_samples[0])\n",
    "    \n",
    "    LDA = LinearDiscriminantAnalysis(sample_dim)\n",
    "    LDA.train(training_samples, training_labels)\n",
    "    pred = LDA.test(testing_samples, testing_labels)\n",
    "    \n",
    "    test_num = len(testing_labels)\n",
    "    correct_num = 0\n",
    "    for i in range(test_num):\n",
    "        if pred[i] == testing_labels[i]:\n",
    "            correct_num += 1\n",
    "\n",
    "    return [test_num, correct_num, correct_num / test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation(samples, labels, multiclass=False, k=5):\n",
    "    \n",
    "    batch_size = int(samples.shape[0] / k)\n",
    "    correct_classification = 0\n",
    "    total = 0\n",
    "    \n",
    "    if multiclass:\n",
    "        for label in set(labels):\n",
    "            print('one vs rest for label_%d: ' % (label))\n",
    "            ones = np.ones_like(labels)\n",
    "            zeros = np.zeros_like(labels)\n",
    "            new_labels = np.where(labels==label, ones, zeros)\n",
    "\n",
    "            for i in range(0, k):\n",
    "                k_train_samples = np.vstack([samples[0 : i * batch_size], samples[(i + 1) * batch_size :]])\n",
    "                k_train_labels = np.hstack([new_labels[0 : i * batch_size], new_labels[(i + 1) * batch_size:]])\n",
    "\n",
    "                k_val_samples = samples[i * batch_size : (i + 1) * batch_size]\n",
    "                k_val_labels = new_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "                res = LDAclassifier(k_train_samples, k_train_labels, k_val_samples, k_val_labels)\n",
    "        \n",
    "                correct_classification += res[1]\n",
    "                total += res[0]\n",
    "                print('ACC of %dth validation : %.3f' % (i, res[2]))\n",
    "                \n",
    "    else:\n",
    "        for i in range(0, k):\n",
    "            k_train_samples = np.vstack([samples[0 : i * batch_size], samples[(i + 1) * batch_size :]])\n",
    "            k_train_labels = np.hstack([labels[0 : i * batch_size], labels[(i + 1) * batch_size:]])\n",
    "\n",
    "            k_val_samples = samples[i * batch_size : (i + 1) * batch_size]\n",
    "            k_val_labels = labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            res = LDAclassifier(k_train_samples, k_train_labels, k_val_samples, k_val_labels)\n",
    "        \n",
    "            correct_classification += res[1]\n",
    "            total += res[0]\n",
    "            print('ACC of %dth validation : %.3f' % (i, res[2]))\n",
    "                    \n",
    "    return correct_classification / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC of 0th validation : 0.667\n",
      "ACC of 1th validation : 1.000\n",
      "ACC of 2th validation : 1.000\n",
      "ACC of 3th validation : 0.667\n",
      "ACC of 4th validation : 0.333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cross_validation(samples, labels, multiclass=False, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, sample_dim):\n",
    "        self.sample_dim = sample_dim\n",
    "        self.threshold = 0.5\n",
    "    \n",
    "    \n",
    "    def train(self, training_samples, training_labels):\n",
    "        sample_dim = self.sample_dim\n",
    "        sample_num = training_samples.shape[0]\n",
    "        if sample_dim != training_samples.shape[1]:\n",
    "            raise Exception(\"Input samples are not compatible with this classifier!\")\n",
    "            \n",
    "            \n",
    "        w = np.ones_like(training_samples[0])\n",
    "        b = 1\n",
    "        prev_w = w\n",
    "        prev_b = b\n",
    "        \n",
    "        lr = 0.001\n",
    "        iteration = 10000\n",
    "        cnt = 0\n",
    "\n",
    "        while cnt < iteration:\n",
    "            prev_w = w\n",
    "            prev_b = b\n",
    "            w = w - lr * self.derivative_over_w(training_labels, training_samples, w, b)\n",
    "            b = b - lr * self.derivative_over_b(training_labels, training_samples, w, b)\n",
    "            if abs(sum(prev_w-w))<1e-5:\n",
    "                break\n",
    "            cnt += 1\n",
    "            \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "            \n",
    "            \n",
    "    def test(self, testing_samples, testing_labels=None):\n",
    "        sample_dim = self.sample_dim\n",
    "        threshold = self.threshold\n",
    "        \n",
    "        sample_num = testing_samples.shape[0]\n",
    "        if sample_dim != testing_samples.shape[1]:\n",
    "            raise Exception(\"Input samples are not compatible with this classifier!\")\n",
    "            \n",
    "        predicted_labels = np.zeros((sample_num))\n",
    "            \n",
    "        w = self.w\n",
    "        b = self.b\n",
    "        \n",
    "        for i in range(sample_num):\n",
    "            xi = testing_samples[i]\n",
    "            \n",
    "            out = self.sigmoid(xi, b, w)\n",
    "            predicted_labels[i] = 1 if out > threshold else 0\n",
    "            \n",
    "        if testing_labels is not None:\n",
    "            acc = accuracy_score(testing_labels, predicted_labels)\n",
    "            \n",
    "        return predicted_labels\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x, b, w):\n",
    "        return 1 / (1 + np.exp(- (w.T).dot(x) - b))\n",
    "    \n",
    "    \n",
    "    def prob_positive(self, w, x, b):\n",
    "        tmp = np.exp((w.T).dot(x) + b)\n",
    "        return tmp / (1 + tmp)\n",
    "    \n",
    "    \n",
    "    def derivative_over_w(self, y, x, w, b):\n",
    "        D = np.zeros_like(x[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            D += (x[i] * y[i] - x[i] * self.prob_positive(w, x[i], b))\n",
    "        return -D\n",
    "    \n",
    "    \n",
    "    def derivative_over_b(self, y, x, w, b):\n",
    "        D = 0\n",
    "        for i in range(x.shape[0]):\n",
    "            D += (y[i] - self.prob_positive(w, x[i], b))\n",
    "        return -D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRclassifier(training_samples, training_labels, testing_samples, testing_labels):\n",
    "    \n",
    "    sample_dim = len(training_samples[0])\n",
    "    \n",
    "    LR = LogisticRegression(sample_dim)\n",
    "    LR.train(training_samples, training_labels)\n",
    "    pred = LR.test(testing_samples, testing_labels)\n",
    "    \n",
    "    test_num = len(testing_labels)\n",
    "    correct_num = 0\n",
    "    for i in range(test_num):\n",
    "        if pred[i] == testing_labels[i]:\n",
    "            correct_num += 1\n",
    "            \n",
    "#     print(confusion_matrix(testing_labels, pred))\n",
    "\n",
    "    return [test_num, correct_num, correct_num / test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation(samples, labels, multiclass=False, k=5):\n",
    "    \n",
    "    batch_size = int(samples.shape[0] / k)\n",
    "    correct_classification = 0\n",
    "    total = 0\n",
    "    \n",
    "    if multiclass:\n",
    "        for label in set(labels):\n",
    "            print('one vs rest for label_%d: ' % (label))\n",
    "            ones = np.ones_like(labels)\n",
    "            zeros = np.zeros_like(labels)\n",
    "            new_labels = np.where(labels==label, ones, zeros)\n",
    "\n",
    "            for i in range(0, k):\n",
    "                k_train_samples = np.vstack([samples[0 : i * batch_size], samples[(i + 1) * batch_size :]])\n",
    "                k_train_labels = np.hstack([new_labels[0 : i * batch_size], new_labels[(i + 1) * batch_size:]])\n",
    "\n",
    "                k_val_samples = samples[i * batch_size : (i + 1) * batch_size]\n",
    "                k_val_labels = new_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "                res = LRclassifier(k_train_samples, k_train_labels, k_val_samples, k_val_labels)\n",
    "        \n",
    "                correct_classification += res[1]\n",
    "                total += res[0]\n",
    "                print('ACC of %dth validation : %.3f' % (i, res[2]))\n",
    "                \n",
    "    else:\n",
    "        for i in range(0, k):\n",
    "            k_train_samples = np.vstack([samples[0 : i * batch_size], samples[(i + 1) * batch_size :]])\n",
    "            k_train_labels = np.hstack([labels[0 : i * batch_size], labels[(i + 1) * batch_size:]])\n",
    "\n",
    "            k_val_samples = samples[i * batch_size : (i + 1) * batch_size]\n",
    "            k_val_labels = labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            res = LRclassifier(k_train_samples, k_train_labels, k_val_samples, k_val_labels)\n",
    "        \n",
    "            correct_classification += res[1]\n",
    "            total += res[0]\n",
    "            print('ACC of %dth validation : %.3f' % (i, res[2]))\n",
    "                    \n",
    "    return correct_classification / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC of 0th validation : 0.667\n",
      "ACC of 1th validation : 1.000\n",
      "ACC of 2th validation : 0.333\n",
      "ACC of 3th validation : 0.667\n",
      "ACC of 4th validation : 0.667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cross_validation(samples, labels, multiclass=False, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NaiveBayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoClassNaiveBayesClassifier():\n",
    "    def __init__(self, sample_dim):\n",
    "        self.sample_dim = sample_dim\n",
    "    \n",
    "    \n",
    "    def train(self, training_samples, flag, training_labels):\n",
    "        sample_dim = self.sample_dim\n",
    "        sample_num = training_samples.shape[0]\n",
    "        if sample_dim != training_samples.shape[1]:\n",
    "            raise Exception(\"Input samples are not compatible with this classifier!\")\n",
    "            \n",
    "        conti_feature = np.where(flag==1)\n",
    "        \n",
    "        dis_feature = np.where(flag==0)\n",
    "        \n",
    "        prob_positive = np.zeros((sample_dim, 3))\n",
    "        prob_negative = np.zeros((sample_dim, 3))\n",
    "        \n",
    "        \n",
    "        mean_list = np.zeros((sample_dim, 2))\n",
    "        std_list = np.zeros((sample_dim, 2))\n",
    "        class_prior = np.zeros((2))\n",
    "        \n",
    "        \n",
    "        class_prior[0] = np.count_nonzero(training_labels==-1) / sample_num\n",
    "        class_prior[1] = np.count_nonzero(training_labels==1) / sample_num\n",
    "        \n",
    "        for dim in conti_feature[0]:\n",
    "            mean_list[dim, 0] = np.mean(training_samples[training_labels==0, dim])\n",
    "            mean_list[dim, 1] = np.mean(training_samples[training_labels==1, dim])\n",
    "            std_list[dim, 0] = np.std(training_samples[training_labels==0, dim])\n",
    "            std_list[dim, 1] = np.std(training_samples[training_labels==1, dim])\n",
    "            \n",
    "        pos = np.where(training_labels==1)\n",
    "        neg = np.where(training_labels==0)\n",
    "        num_positive = len(pos[0])\n",
    "        num_negative = len(neg[0])\n",
    "        \n",
    "        for dim in dis_feature[0]:\n",
    "            feature = training_samples[training_labels==1][:,dim]\n",
    "            nums = np.unique(feature)\n",
    "            for i in range(len(nums)):\n",
    "                prob_positive[dim][i] = feature[feature==nums[i]].shape[0] / num_positive\n",
    "                \n",
    "            feature = training_samples[training_labels==-1][:,dim]\n",
    "            nums = np.unique(feature)\n",
    "            for i in range(len(nums)):\n",
    "                prob_negative[dim][i] = feature[feature==nums[i]].shape[0] / num_negative\n",
    "            \n",
    "        self.class_prior = class_prior\n",
    "        self.mean_list = mean_list\n",
    "        self.std_list = std_list\n",
    "        self.prob_positive = prob_positive\n",
    "        self.prob_negative = prob_negative\n",
    "        \n",
    "    \n",
    "    def test(self, testing_samples, flag, testing_labels=None):\n",
    "        \n",
    "        sample_dim = self.sample_dim\n",
    "        sample_num = testing_samples.shape[0]\n",
    "        if sample_dim != testing_samples.shape[1]:\n",
    "            raise Exception(\"Input samples are not compatible with this classifier!\")\n",
    "            \n",
    "        predicted_labels = np.zeros((sample_num))\n",
    "        \n",
    "        class_prior = self.class_prior\n",
    "        mean_list = self.mean_list\n",
    "        std_list = self.std_list\n",
    "        prob_positive = self.prob_positive\n",
    "        prob_negative = self.prob_negative\n",
    "        \n",
    "        conti_feature = np.where(flag==1)\n",
    "        dis_feature = np.where(flag==0)\n",
    "        \n",
    "        for i in range(sample_num):\n",
    "            xi = testing_samples[i]\n",
    "            \n",
    "            xi_posterior_prob = [1, 1]\n",
    "            \n",
    "            for dim in conti_feature[0]:\n",
    "                xi_prob1 = self.Gaussian(xi[dim], mean_list[dim, 0], std_list[dim, 0])\n",
    "                xi_posterior_prob[0] *= xi_prob1\n",
    "                \n",
    "                xi_prob2 = self.Gaussian(xi[dim], mean_list[dim, 1], std_list[dim, 1])\n",
    "                xi_posterior_prob[1] *= xi_prob2\n",
    "                \n",
    "            for dim in dis_feature[0]:\n",
    "\n",
    "                xi_prob1 = prob_negative[dim][int(xi[dim])-1]\n",
    "                xi_posterior_prob[0] *= xi_prob1\n",
    "                \n",
    "                xi_prob2 = prob_positive[dim][int(xi[dim])-1]\n",
    "                xi_posterior_prob[1] *= xi_prob2\n",
    "                \n",
    "            xi_posterior_prob[0] *= class_prior[0]\n",
    "            xi_posterior_prob[1] *= class_prior[1]\n",
    "            \n",
    "            if xi_posterior_prob[0] > xi_posterior_prob[1]:\n",
    "                predicted_labels[i] = 0\n",
    "            else:\n",
    "                predicted_labels[i] = 1\n",
    "                \n",
    "        if testing_labels is not None:\n",
    "            acc = accuracy_score(testing_labels, predicted_labels)\n",
    "                \n",
    "        return predicted_labels\n",
    "                \n",
    "                \n",
    "    def Gaussian(self, x, mean, std):\n",
    "        return np.exp(- 1 / 2 * np.dot((x - mean).T, (x - mean)) / std) / (2 * np.pi * np.sqrt(np.abs(std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBClassifier(training_samples, training_labels, testing_samples, testing_labels, flag):\n",
    "    \n",
    "    sample_dim = len(flag)\n",
    "    \n",
    "    NBC = TwoClassNaiveBayesClassifier(sample_dim)\n",
    "    NBC.train(training_samples, flag, training_labels)\n",
    "    pred = NBC.test(testing_samples, flag, testing_labels)\n",
    "    \n",
    "    test_num = len(testing_labels)\n",
    "    correct_num = 0\n",
    "    for i in range(test_num):\n",
    "        if pred[i] == testing_labels[i]:\n",
    "            correct_num += 1\n",
    "            \n",
    "    return [test_num, correct_num, correct_num / test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation(samples, labels, flag, k=5):\n",
    "    \n",
    "    batch_size = int(samples.shape[0] / k)\n",
    "    correct_classification = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, k):\n",
    "        k_train_samples = np.vstack([samples[0 : i * batch_size], samples[(i + 1) * batch_size :]])\n",
    "        k_train_labels = np.hstack([labels[0 : i * batch_size], labels[(i + 1) * batch_size:]])\n",
    "\n",
    "        k_val_samples = samples[i * batch_size : (i + 1) * batch_size]\n",
    "        k_val_labels = labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        res = NBClassifier(k_train_samples, k_train_labels, k_val_samples, k_val_labels, flag)\n",
    "        \n",
    "        correct_classification += res[1]\n",
    "        total += res[0]\n",
    "        print('ACC of %dth validation : %.3f' % (i, res[2]))\n",
    "                    \n",
    "    return correct_classification / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC of 0th validation : 0.333\n",
      "ACC of 1th validation : 0.333\n",
      "ACC of 2th validation : 0.667\n",
      "ACC of 3th validation : 0.667\n",
      "ACC of 4th validation : 0.333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4666666666666667"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cross_validation(samples, labels, flag, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMclassifier(kernel, training_samples, training_labels, testing_samples, testing_labels):\n",
    "    clf = svm.SVC(kernel=kernel, C=0.1)\n",
    "    clf.fit(training_samples, training_labels)\n",
    "    pred = clf.predict(testing_samples)\n",
    "    \n",
    "    test_num = len(testing_labels)\n",
    "    correct_num = 0\n",
    "    for i in range(test_num):\n",
    "        if pred[i] == testing_labels[i]:\n",
    "            correct_num += 1\n",
    "            \n",
    "    return [test_num, correct_num, correct_num / test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation(samples, labels, k=5, kernel='linear'):\n",
    "    \n",
    "    batch_size = int(samples.shape[0] / k)\n",
    "    correct_classification = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, k):\n",
    "        k_train_samples = np.vstack([samples[0 : i * batch_size], samples[(i + 1) * batch_size :]])\n",
    "        k_train_labels = np.hstack([labels[0 : i * batch_size], labels[(i + 1) * batch_size:]])\n",
    "\n",
    "        k_val_samples = samples[i * batch_size : (i + 1) * batch_size]\n",
    "        k_val_labels = labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        res = SVMclassifier(kernel, k_train_samples, k_train_labels, k_val_samples, k_val_labels)\n",
    "        \n",
    "        correct_classification += res[1]\n",
    "        total += res[0]\n",
    "        print('ACC of %dth validation : %.3f' % (i, res[2]))\n",
    "        \n",
    "    print('total acc: %.3f' % (correct_classification / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: linear\n",
      "ACC of 0th validation : 0.667\n",
      "ACC of 1th validation : 0.667\n",
      "ACC of 2th validation : 1.000\n",
      "ACC of 3th validation : 0.667\n",
      "ACC of 4th validation : 0.333\n",
      "total acc: 0.667\n",
      "kernel: rbf\n",
      "ACC of 0th validation : 0.667\n",
      "ACC of 1th validation : 0.667\n",
      "ACC of 2th validation : 0.333\n",
      "ACC of 3th validation : 0.333\n",
      "ACC of 4th validation : 0.333\n",
      "total acc: 0.467\n",
      "kernel: poly\n",
      "ACC of 0th validation : 0.667\n",
      "ACC of 1th validation : 1.000\n",
      "ACC of 2th validation : 1.000\n",
      "ACC of 3th validation : 0.667\n",
      "ACC of 4th validation : 0.333\n",
      "total acc: 0.733\n",
      "kernel: sigmoid\n",
      "ACC of 0th validation : 0.333\n",
      "ACC of 1th validation : 0.000\n",
      "ACC of 2th validation : 0.333\n",
      "ACC of 3th validation : 0.333\n",
      "ACC of 4th validation : 0.000\n",
      "total acc: 0.200\n"
     ]
    }
   ],
   "source": [
    "for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "    print('kernel: %s' % (kernel))\n",
    "    Cross_validation(samples, labels, k=5, kernel=kernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
